"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.prependComputeIxs = exports.MAX_COMPUTE_UNITS = exports.getTransactionConvertedToLegacy = exports.convertTxToLegacy = exports.getAccountKeys = exports.getMultipleAccountsWaitSlot = exports.getAccountWaitSlot = exports.legacyToV0Tx = exports.serializeAnyVersionTx = exports.getLatestBlockHeight = exports.confirmTransactionMultConns = exports.getLatestBlockhashMultConns = exports.buildTxsLegacyV0 = exports.buildTxV0 = exports.buildTx = exports.RetryTxSender = exports.castMessageJSON = exports.castTxResponse = exports.castTxResponseJSON = void 0;
const web3_js_1 = require("@solana/web3.js");
const assert_1 = __importDefault(require("assert"));
const bs58_1 = __importDefault(require("bs58"));
const exponential_backoff_1 = require("exponential-backoff");
const time_1 = require("../time");
const utils_1 = require("../utils");
const BLOCK_TIME_MS = 400;
const DEFAULT_CONFIRM_OPTS = {
    commitment: 'confirmed',
    //even if we're skipping preflight, this should be set to the same level as committment above
    //as per https://jstarry.notion.site/Transaction-confirmation-d5b8f4e09b9c4a70a1f263f82307d7ce
    preflightCommitment: 'confirmed',
    skipPreflight: true,
};
const DEFAULT_TIMEOUT_MS = 60000;
const DEFAULT_RETRY_MS = 2000;
const MAX_WAIT_BLOCKHEIGHT_MS = 10 * 1000;
const castTxResponseJSON = (tx) => {
    return {
        ...tx,
        transaction: {
            ...tx.transaction,
            message: (0, exports.castMessageJSON)(tx.transaction.message),
        },
    };
};
exports.castTxResponseJSON = castTxResponseJSON;
const castTxResponse = (tx) => {
    return {
        ...tx,
        transaction: {
            ...tx.transaction,
            message: new web3_js_1.Message(tx.transaction.message),
        },
    };
};
exports.castTxResponse = castTxResponse;
const castMessageJSON = (msg) => {
    return {
        ...msg,
        accountKeys: msg.accountKeys.map((k) => k.toBase58()),
    };
};
exports.castMessageJSON = castMessageJSON;
class RetryTxSender {
    done = false;
    resolveReference = {
        resolve: undefined,
    };
    cancelReference = {
        resolve: undefined,
    };
    start;
    txSig;
    confirmedTx;
    connection;
    additionalConnections;
    logger;
    opts;
    timeout;
    retrySleep;
    constructor({ connection, additionalConnections = new Array(), logger, txSig, opts = DEFAULT_CONFIRM_OPTS, timeout = DEFAULT_TIMEOUT_MS, retrySleep = DEFAULT_RETRY_MS, }) {
        this.connection = connection;
        this.additionalConnections = additionalConnections;
        this.logger = logger;
        this.txSig = txSig;
        this.opts = opts;
        this.timeout = timeout;
        this.retrySleep = retrySleep;
    }
    async send(tx) {
        const rawTransaction = tx.serialize();
        const startTime = this._getTimestamp();
        try {
            this.txSig = await this.connection.sendRawTransaction(rawTransaction, this.opts);
            this.logger?.info(`Begin processing: ${this.txSig}`);
            this.logger?.info(`üöÄ [${this.txSig.substring(0, 5)}] tx sent to MAIN connection`);
            this._sendToAdditionalConnections(rawTransaction);
        }
        catch (e) {
            this.logger?.error(`${JSON.stringify(e)}`);
            throw e;
        }
        //asynchronously keep retrying until done or timeout
        (async () => {
            while (!this.done && this._getTimestamp() - startTime < this.timeout) {
                this.logger?.info(`üîÅ [${this.txSig?.substring(0, 5)}] begin new retry loop (sleeping for ${this.retrySleep / 1000}s)`);
                await this._sleep();
                if (!this.done) {
                    this.connection
                        .sendRawTransaction(rawTransaction, this.opts)
                        .catch((e) => {
                        this.logger?.error(`${JSON.stringify(e)}`);
                        this._stopWaiting();
                    });
                    this._sendToAdditionalConnections(rawTransaction);
                }
            }
        })();
        return this.txSig;
    }
    async tryConfirm(lastValidBlockHeight, opts) {
        if (this.confirmedTx) {
            this.logger?.info('‚úÖ Tx already confirmed');
            return this.confirmedTx;
        }
        if (!this.txSig) {
            throw new Error('you need to send the tx first');
        }
        try {
            const result = await this._confirmTransaction(this.txSig, lastValidBlockHeight, opts);
            this.confirmedTx = {
                txSig: this.txSig,
                slot: result.context.slot,
                err: result.value.err,
            };
            return this.confirmedTx;
        }
        catch (e) {
            this.logger?.error(`${JSON.stringify(e)}`);
            throw e;
        }
        finally {
            this._stopWaiting();
        }
    }
    cancelConfirm() {
        if (this.cancelReference.resolve) {
            this.cancelReference.resolve();
        }
    }
    async _confirmTransaction(txSig, lastValidBlockHeight, opts) {
        this.logger?.info(`‚è≥ [${txSig.substring(0, 5)}] begin trying to confirm tx`);
        let decodedSignature;
        try {
            decodedSignature = bs58_1.default.decode(txSig);
        }
        catch (err) {
            throw new Error('signature must be base58 encoded: ' + txSig);
        }
        (0, assert_1.default)(decodedSignature.length === 64, 'signature has invalid length');
        this.start = Date.now();
        const subscriptionCommitment = this.opts.commitment;
        const subscriptionIds = new Array();
        const wsResolveRefs = new Array();
        const connections = [this.connection, ...this.additionalConnections];
        let response = null;
        const promises = connections
            .map((connection, i) => {
            let subscriptionId;
            const pollPromise = (0, exponential_backoff_1.backOff)(async () => {
                this.logger?.debug('[getSignatureStatus] Attept to get sig status');
                const { value, context } = await connection.getSignatureStatus(txSig, {
                    searchTransactionHistory: true,
                });
                if (!value) {
                    this.logger?.debug(`[getSignatureStatus] sig status for ${txSig} not found, try again in ${this.retrySleep}`);
                    throw new Error(`sig status for ${txSig} not found`);
                }
                // This is possible, and the slot may != confirmed slot if minority node processed it.
                if (value.confirmationStatus === 'processed') {
                    this.logger?.debug(`[getSignatureStatus] sig status for ${txSig} still in processed state, try again in ${this.retrySleep}`);
                    throw new Error(`sig status for ${txSig} still in processed state`);
                }
                return {
                    value,
                    context,
                };
            }, {
                maxDelay: this.retrySleep,
                startingDelay: this.retrySleep,
                numOfAttempts: Math.ceil(this.timeout / this.retrySleep),
                retry: (e) => {
                    console.error(`[getSignatureStatus] received error, ${e} retrying`);
                    return !this.done;
                },
            })
                .then((res) => {
                response = res;
            })
                .catch((err) => {
                this.logger?.error(`[${txSig.substring(0, 5)}] error polling: ${err}`);
            });
            if (opts?.disableWs)
                return [pollPromise];
            const wsPromise = new Promise((resolve) => {
                try {
                    wsResolveRefs.push({ resolve: () => resolve(null) });
                    subscriptionId = connection.onSignature(txSig, (result, context) => {
                        subscriptionIds[i] = undefined;
                        response = {
                            context,
                            value: result,
                        };
                        resolve(null);
                    }, subscriptionCommitment);
                }
                catch (err) {
                    this.logger?.error(`[${txSig.substring(0, 5)}] error setting up onSig WS: ${err}`);
                    // Don't want this to cause everything else to fail during race.
                    resolve(null);
                }
            });
            subscriptionIds.push(subscriptionId);
            return [wsPromise, pollPromise];
        })
            .flat();
        try {
            await this._racePromises(txSig, promises, this.timeout, lastValidBlockHeight);
        }
        finally {
            for (const [i, subscriptionId] of subscriptionIds.entries()) {
                if (!(0, utils_1.isNullLike)(subscriptionId)) {
                    connections[i].removeSignatureListener(subscriptionId);
                }
            }
            wsResolveRefs.forEach((resolveRef) => resolveRef.resolve?.());
        }
        const duration = (Date.now() - this.start) / 1000;
        if (response === null) {
            const errMsg = `‚ùå [${txSig.substring(0, 5)}] NOT confirmed in ${duration.toFixed(2)}sec`;
            this.logger?.error(errMsg);
            throw new Error(errMsg);
        }
        if (response.value.err) {
            this.logger?.warn(`‚ö†Ô∏è [${txSig.substring(0, 5)}] confirmed AS FAILED TX in ${duration.toFixed(2)}sec`);
        }
        else {
            this.logger?.info(`‚úÖ [${txSig.substring(0, 5)}] confirmed in ${duration.toFixed(2)}sec`);
        }
        return response;
    }
    _getTimestamp() {
        return new Date().getTime();
    }
    _stopWaiting() {
        this.done = true;
        if (this.resolveReference.resolve) {
            this.resolveReference.resolve();
        }
    }
    async _sleep() {
        return new Promise((resolve) => {
            this.resolveReference.resolve = resolve;
            setTimeout(resolve, this.retrySleep);
        });
    }
    //this will trigger faster than a timeout promise in the following situations:
    // 1)we've set too long of a timeout, typically > 90s
    // 2)the blockhash we got is outdated and eg is actually only valid for 10s, when timeout is 60s
    // 3)the validator is confirming blocks faster than 400ms, eg 200ms (possible, confirmed with Jacob) -> 151 slots will fly by faster
    async _outdatedBlockHeightPromise(lastValidBlockHeight) {
        let currentHeight = await (0, exports.getLatestBlockHeight)({
            connections: [this.connection, ...this.additionalConnections],
        });
        while (!this.done && currentHeight < lastValidBlockHeight) {
            const waitMs = Math.min((lastValidBlockHeight - currentHeight) * BLOCK_TIME_MS, MAX_WAIT_BLOCKHEIGHT_MS);
            this.logger?.debug(`current height is ${lastValidBlockHeight - currentHeight} below lastValidBlockHeight, waiting ${waitMs}ms before checking again...`);
            await (0, time_1.waitMS)(waitMs);
            currentHeight = await (0, exports.getLatestBlockHeight)({
                connections: [this.connection, ...this.additionalConnections],
            });
        }
        if (currentHeight > lastValidBlockHeight) {
            this.logger?.error(`‚ùå [${this.txSig?.substring(0, 5)}] current height ${currentHeight - lastValidBlockHeight} blocks > lastValidBlockHeight, aborting`);
        }
        return null;
    }
    _racePromises(txSig, promises, timeoutMs, lastValidBlockHeight) {
        let timeoutId;
        let timeoutResolve;
        const timeoutPromise = new Promise((resolve) => {
            timeoutResolve = resolve;
            timeoutId = setTimeout(() => {
                this.logger?.warn(`[${txSig}] timeout waiting for sig`);
                resolve(null);
            }, timeoutMs);
        });
        let cancelResolve;
        const cancelPromise = new Promise((resolve) => {
            cancelResolve = resolve;
            this.cancelReference.resolve = () => {
                const errMsg = `[${txSig}] ‚ö†Ô∏è confirmation CANCELLED`;
                this.logger?.warn(errMsg);
                resolve(null);
            };
        });
        const promisesToRace = [...promises, timeoutPromise, cancelPromise];
        if (lastValidBlockHeight) {
            promisesToRace.push(this._outdatedBlockHeightPromise(lastValidBlockHeight));
        }
        return Promise.race(promisesToRace).then((result) => {
            timeoutResolve(null);
            clearTimeout(timeoutId);
            cancelResolve(null);
            return result;
        });
    }
    _sendToAdditionalConnections(rawTx) {
        this.additionalConnections.map((connection) => {
            connection.sendRawTransaction(rawTx, this.opts).catch((e) => {
                this.logger?.error(
                // @ts-ignore
                `error sending tx to additional connection ${connection._rpcEndpoint}: ${e}`);
            });
        });
        this.logger?.info(`üí• [${this.txSig?.substring(0, 5)}] tx sent to ${this.additionalConnections.length} ADDITIONAL connections`);
    }
    addAdditionalConnection(newConnection) {
        const alreadyUsingConnection = this.additionalConnections.filter((connection) => {
            return connection.rpcEndpoint === newConnection.rpcEndpoint;
        }).length > 0;
        if (!alreadyUsingConnection) {
            this.additionalConnections.push(newConnection);
        }
    }
}
exports.RetryTxSender = RetryTxSender;
const maybeFetchBlockhash = async (maybeBlockhash) => {
    let blockhash;
    let lastValidBlockHeight;
    if (maybeBlockhash.type === 'blockhash') {
        blockhash = maybeBlockhash.blockhash;
        lastValidBlockHeight = null;
    }
    else {
        const { blockhash: blockhash_, lastValidBlockHeight: lastValidBlockHeight_, } = await (0, exports.getLatestBlockhashMultConns)(maybeBlockhash.args);
        blockhash = blockhash_;
        lastValidBlockHeight = lastValidBlockHeight_;
    }
    return { blockhash, lastValidBlockHeight };
};
//(!) this should be the only function across our code used to build txs
// reason: we want to control how blockchash is constructed to minimize tx failures
const buildTx = async ({ feePayer, instructions, additionalSigners, maybeBlockhash, }) => {
    if (!instructions.length) {
        throw new Error('must pass at least one instruction');
    }
    const tx = new web3_js_1.Transaction();
    tx.add(...instructions);
    tx.feePayer = feePayer;
    const { blockhash, lastValidBlockHeight } = await maybeFetchBlockhash(maybeBlockhash);
    tx.recentBlockhash = blockhash;
    if (!!lastValidBlockHeight) {
        tx.lastValidBlockHeight = lastValidBlockHeight;
    }
    if (additionalSigners) {
        additionalSigners
            .filter((s) => s !== undefined)
            .forEach((kp) => {
            tx.partialSign(kp);
        });
    }
    return { tx, blockhash, lastValidBlockHeight };
};
exports.buildTx = buildTx;
const buildTxV0 = async ({ feePayer, instructions, additionalSigners, addressLookupTableAccs, maybeBlockhash, }) => {
    if (!instructions.length) {
        throw new Error('must pass at least one instruction');
    }
    const { blockhash, lastValidBlockHeight } = await maybeFetchBlockhash(maybeBlockhash);
    const msg = new web3_js_1.TransactionMessage({
        payerKey: feePayer,
        recentBlockhash: blockhash,
        instructions,
    }).compileToV0Message(addressLookupTableAccs);
    const tx = new web3_js_1.VersionedTransaction(msg);
    if (additionalSigners) {
        tx.sign(additionalSigners.filter((s) => s !== undefined));
    }
    return { tx, blockhash, lastValidBlockHeight };
};
exports.buildTxV0 = buildTxV0;
const buildTxsLegacyV0 = async ({ feePayer, instructions, additionalSigners, addressLookupTableAccs, maybeBlockhash, }) => {
    if (!instructions.length) {
        throw new Error('must pass at least one instruction');
    }
    const { blockhash, lastValidBlockHeight } = await maybeFetchBlockhash(maybeBlockhash);
    const msg = new web3_js_1.TransactionMessage({
        payerKey: feePayer,
        recentBlockhash: blockhash,
        instructions,
    });
    const tx = new web3_js_1.Transaction().add(...instructions);
    tx.recentBlockhash = blockhash;
    tx.feePayer = feePayer;
    if (lastValidBlockHeight) {
        tx.lastValidBlockHeight = lastValidBlockHeight;
    }
    const txV0 = new web3_js_1.VersionedTransaction(msg.compileToV0Message(addressLookupTableAccs));
    if (additionalSigners) {
        const signers = (0, utils_1.filterNullLike)(additionalSigners);
        signers.forEach((kp) => {
            tx.partialSign(kp);
        });
        txV0.sign(signers);
    }
    return { tx, txV0, blockhash, lastValidBlockHeight };
};
exports.buildTxsLegacyV0 = buildTxsLegacyV0;
const getLatestBlockhashMultConns = async ({ connections, commitment = 'confirmed', maxRetries = 5, startTimeoutMs = 100, maxTimeoutMs = 2000, }) => {
    let retries = 0;
    let timeoutMs = startTimeoutMs;
    let blockhashes = [];
    while (blockhashes.length < 1 && retries < maxRetries) {
        //poll blockhashes from multiple providers, then take the one from RPC with latest slot
        //as per advice here https://jstarry.notion.site/Transaction-confirmation-d5b8f4e09b9c4a70a1f263f82307d7ce
        blockhashes = await (0, utils_1.settleAllWithTimeout)(connections.map((c) => c.getLatestBlockhashAndContext(commitment)), timeoutMs);
        retries++;
        timeoutMs = Math.min(timeoutMs * 2, maxTimeoutMs);
    }
    if (!blockhashes.length) {
        throw new Error(`failed to fetch blockhash from ${connections.length} providers`);
    }
    // (!) Regression: sorting by slot is not enough: eg a stale blockhash RPC can still return a newer
    // slot than the rest. lastValidBlockHeight should correspond 1:1.
    return blockhashes.sort((a, b) => b.value.lastValidBlockHeight - a.value.lastValidBlockHeight)[0].value;
};
exports.getLatestBlockhashMultConns = getLatestBlockhashMultConns;
/**
 * Races multiple connections to confirm a tx.
 *
 * This will throw TransactionExpiredBlockheightExceededError
 * if we cannot confirm by the tx's lastValidBlockHeight.
 */
const confirmTransactionMultConns = async ({ conns, sig, timeoutMs = 60 * 1000, maxDelayMs = 10 * 1000, startingDelayMs = 3 * 1000, numOfAttempts = 7, }) => {
    return await Promise.race([
        new Promise((_, rej) => setTimeout(() => rej(new Error(`confirming ${sig} timeout exceed ${timeoutMs}ms`)), timeoutMs)),
        // LOL this doesn't actually work for old sigs wtf.
        // ...conns.map(async (c) =>
        //   // Backoff in case one of the RPCs is acting up: hopefully the other will out-race a confirmation.
        //   backOff(() => c.confirmTransaction(args), {
        //     retry: (e) => {
        //       return !(e instanceof TransactionExpiredBlockheightExceededError);
        //     },
        //   })
        // ),
        ...conns.map(async (c) => (0, exponential_backoff_1.backOff)(async () => {
            const { value, context } = await c.getSignatureStatus(sig, {
                searchTransactionHistory: true,
            });
            if (!value)
                throw new Error(`sig status for ${sig} not found`);
            // This is possible, and the slot may != confirmed slot if minority node processed it.
            if (value.confirmationStatus === 'processed')
                throw new Error(`sig status for ${sig} still in processed state`);
            return {
                value,
                context,
            };
        }, {
            maxDelay: maxDelayMs,
            startingDelay: startingDelayMs,
            numOfAttempts,
        })),
    ]);
};
exports.confirmTransactionMultConns = confirmTransactionMultConns;
const getLatestBlockHeight = async ({ connections, commitment = 'confirmed', maxRetries = 5, startTimeoutMs = 100, maxTimeoutMs = 2000, }) => {
    let retries = 0;
    let timeoutMs = startTimeoutMs;
    let heights = [];
    while (heights.length < 1 && retries < maxRetries) {
        //poll heights from multiple providers, then take the one from RPC with latest slot
        //as per advice here https://jstarry.notion.site/Transaction-confirmation-d5b8f4e09b9c4a70a1f263f82307d7ce
        heights = await (0, utils_1.settleAllWithTimeout)(connections.map((c) => c.getBlockHeight(commitment)), timeoutMs);
        retries++;
        timeoutMs = Math.min(timeoutMs * 2, maxTimeoutMs);
    }
    if (!heights.length) {
        throw new Error(`failed to fetch height from ${connections.length} providers`);
    }
    return Math.max(...heights);
};
exports.getLatestBlockHeight = getLatestBlockHeight;
//Do NOT use Uint8Array as output, this would cause all kinds of serialization problems in graphql
const serializeAnyVersionTx = (tx, verifySignatures = false) => {
    if (tx instanceof web3_js_1.Transaction) {
        return tx.serialize({ verifySignatures }).toJSON().data;
    }
    else if (tx instanceof web3_js_1.VersionedTransaction) {
        //verify signatures = always false
        return Array.from(tx.serialize());
    }
    else {
        // This is to handle weird wallet adapters that don't return Transaction/VersionedTransaction objects.
        const unkTx = tx;
        try {
            return unkTx.serialize({ verifySignatures }).toJSON().data;
        }
        catch (err) {
            console.error(err);
            try {
                return Array.from(unkTx.serialize());
            }
            catch (err) {
                console.error(err);
                throw new Error('unknown tx type');
            }
        }
    }
};
exports.serializeAnyVersionTx = serializeAnyVersionTx;
const legacyToV0Tx = (legacy) => {
    return new web3_js_1.VersionedTransaction(web3_js_1.Transaction.from(legacy).compileMessage());
};
exports.legacyToV0Tx = legacyToV0Tx;
const MIN_SLOT_MS = 400;
// In case we get a flaky slot from a bad RPC (o/w we may end up waiting A LONG time = stuck).
const MAX_WAIT_UNTIL_MS = 5 * 1000;
/** Use this vs getAccountInfo w/ minContextSlot since minContextSlot just spam retries.
 * See getMultiAccountsWaitSlot if you have a batch of accounts.  */
const getAccountWaitSlot = async ({ conn, slot, pubkey, beforeHook, retries = 5, }) => {
    let curSlot;
    let account;
    while (true) {
        ({
            context: { slot: curSlot },
            // Will be null if account cannot be found.
            value: account,
        } = await (0, exponential_backoff_1.backOff)(async () => {
            await beforeHook?.();
            return await conn.getAccountInfoAndContext(pubkey);
        }, {
            numOfAttempts: retries,
        }));
        // Need to wait for slot AFTER the tx.
        if (curSlot > slot)
            break;
        const interval = Math.min(Math.ceil(Math.max(1, slot - curSlot) * MIN_SLOT_MS), MAX_WAIT_UNTIL_MS);
        console.warn(`retrieve pda ${pubkey.toBase58()} with pda slot ${curSlot} <= tx slot ${slot}, waiting ${interval}ms and retrying...`);
        await (0, time_1.sleep)({ Millis: interval });
    }
    return { slot: curSlot, account, pubkey };
};
exports.getAccountWaitSlot = getAccountWaitSlot;
const getMultipleAccountsWaitSlot = async ({ conn, slot, pubkeys, beforeHook, retries = 5, }) => {
    return (await Promise.all((0, utils_1.makeBatches)(pubkeys, 100).map(async (batch, batchIdx) => {
        let curSlot;
        let accounts;
        while (true) {
            ({
                context: { slot: curSlot },
                // Will be null if account cannot be found.
                value: accounts,
            } = await (0, exponential_backoff_1.backOff)(async () => {
                await beforeHook?.();
                return await conn.getMultipleAccountsInfoAndContext(batch);
            }, {
                numOfAttempts: retries,
            }));
            // Need to wait for slot AFTER the tx.
            if (curSlot > slot)
                break;
            const interval = Math.min(Math.ceil(Math.max(1, slot - curSlot) * MIN_SLOT_MS), MAX_WAIT_UNTIL_MS);
            console.warn(`retrieve pda for ${batch.length} keys (batch ${batchIdx}, first: ${batch[0].toBase58()}) with pda slot ${curSlot} <= tx slot ${slot}, waiting ${interval}ms and retrying...`);
            await (0, time_1.sleep)({ Millis: interval });
        }
        return accounts.map((account, idx) => ({
            slot: curSlot,
            account,
            pubkey: batch[idx],
        }));
    }))).flat();
};
exports.getMultipleAccountsWaitSlot = getMultipleAccountsWaitSlot;
// NB: use this o/w the getAccountKey fn on VersionedTransactionResponse from Geyser/RPC doesn't work...
const getAccountKeys = (tx) => {
    return [
        ...(tx.transaction.message.staticAccountKeys ?? []),
        ...(tx.meta?.loadedAddresses?.writable ?? []),
        ...(tx.meta?.loadedAddresses?.readonly ?? []),
    ];
};
exports.getAccountKeys = getAccountKeys;
/** converts the new v0 tx type to legacy so that our downstream parser works as expected */
const convertTxToLegacy = (tx) => {
    // Okay this is really fucking weird, but here is the observed behavior:
    // JSON RPC getTransaction:
    // - legacy: in TransactionResponse format
    // - v0: in VersionedTransactionResponse format
    // Geyser SQS:
    // - legacy & v0 in VersionedTransactionResponse
    //handle TransactionResponse/legacy format, return as is
    if ((tx.version === undefined ||
        tx.version === null ||
        tx.version === 'legacy') &&
        !('compiledInstructions' in tx.transaction.message)) {
        return tx;
    }
    //handle VersionedTransactionResponse
    const v0msg = tx.transaction.message;
    const legacyMsg = new web3_js_1.Message({
        header: v0msg.header,
        accountKeys: (0, exports.getAccountKeys)(tx),
        instructions: v0msg.compiledInstructions.map((i) => {
            const { accountKeyIndexes, ...rest } = i;
            return {
                ...rest,
                accounts: accountKeyIndexes,
                //when parsing a JSON file, this field is a stringified buffer ({type: Buffer, data: [1,2,3]})
                data: 'data' in i.data
                    ? bs58_1.default.encode(Uint8Array.from(i.data.data))
                    : bs58_1.default.encode(i.data),
            };
        }),
        recentBlockhash: v0msg.recentBlockhash,
    });
    return {
        ...tx,
        meta: tx.meta
            ? {
                ...tx.meta,
                loadedAddresses: {
                    readonly: [],
                    writable: [],
                },
            }
            : null,
        transaction: {
            ...tx.transaction,
            message: legacyMsg,
        },
        v0LoadedAddresses: tx.v0LoadedAddresses ?? {
            numReadonlyAccounts: tx.meta?.loadedAddresses?.readonly?.length ?? 0,
            numWritableAccounts: tx.meta?.loadedAddresses?.writable?.length ?? 0,
        },
    };
};
exports.convertTxToLegacy = convertTxToLegacy;
/** gets new v0 and legacy transactions with the old TransactionResponse format */
const getTransactionConvertedToLegacy = async (conn, sig, commitment = 'confirmed') => {
    const tx = await conn.getTransaction(sig, {
        commitment,
        maxSupportedTransactionVersion: 0,
    });
    if (!tx)
        return null;
    return (0, exports.convertTxToLegacy)(tx);
};
exports.getTransactionConvertedToLegacy = getTransactionConvertedToLegacy;
// Current max compute per tx.
exports.MAX_COMPUTE_UNITS = 1400000;
/** Adds (1) increase compute + (2) priority fees */
const prependComputeIxs = (ixs, compute, priorityMicroLamports) => {
    const out = [...ixs];
    if (compute &&
        !ixs.some((ix) => ix.programId.equals(web3_js_1.ComputeBudgetProgram.programId) &&
            web3_js_1.ComputeBudgetInstruction.decodeInstructionType(ix) ===
                'SetComputeUnitLimit')) {
        const modifyComputeUnits = web3_js_1.ComputeBudgetProgram.setComputeUnitLimit({
            units: Math.min(exports.MAX_COMPUTE_UNITS, compute),
        });
        out.splice(0, 0, modifyComputeUnits);
    }
    if (priorityMicroLamports &&
        !ixs.some((ix) => ix.programId.equals(web3_js_1.ComputeBudgetProgram.programId) &&
            web3_js_1.ComputeBudgetInstruction.decodeInstructionType(ix) ===
                'SetComputeUnitPrice')) {
        const addPriorityFee = web3_js_1.ComputeBudgetProgram.setComputeUnitPrice({
            microLamports: priorityMicroLamports,
        });
        out.splice(0, 0, addPriorityFee);
    }
    return out;
};
exports.prependComputeIxs = prependComputeIxs;
//# sourceMappingURL=transaction.js.map